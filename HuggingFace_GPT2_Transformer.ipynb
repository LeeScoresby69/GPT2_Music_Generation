{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeScoresby69/GPT2_Music_Generation/blob/main/HuggingFace_GPT2_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISst4W6VKIWt"
      },
      "source": [
        "![alt text](./LMU_Kopf.png \"LMU\")\n",
        "\n",
        "# Masterarbeit\n",
        "\n",
        "## im Studiengang\n",
        "## Pädagogik mit Schwerpunkt Bildungsforschung und Bildungsmanagement\n",
        "## an der LMU München\n",
        "\n",
        "\n",
        "### Analyse und Generierung von motivischen Sequenzdaten – Eine Studie anhand von transformationsbasierten maschinellen Lernverfahren\n",
        "\n",
        "\n",
        "4. Fachsemester\n",
        "\n",
        "\n",
        "Verfasserin: |   | Betreuer:\n",
        "--- | --- | ---\n",
        " Laura Katharina Achatz |       | Prof. Dr. Marcus Spies\n",
        " Martrikelnummer: 11729089 |     | Raum 3107\n",
        " Rosenstraße 6 |   | Leopoldstraße 13\n",
        " 85778 Haimhausen |   | 80802 München\n",
        " laura.achatz@campus.lmu.de |    | marcus.spies@lmu.de\n",
        "\n",
        "\n",
        "Abgabedatum: 11.09.2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiTIpPjArIyr"
      },
      "source": [
        "# HuggingFace GPT2\n",
        "\n",
        "$$\n",
        "@inproceedings{miditok2021,\n",
        "    title={{MidiTok}: A Python package for {MIDI} file tokenization},\n",
        "    author={Fradet, Nathan and Briot, Jean-Pierre and Chhel, Fabien and El Fallah Seghrouchni, Amal and Gutowski, Nicolas},\n",
        "    booktitle={Extended Abstracts for the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference},\n",
        "    year={2021},\n",
        "    url={https://archives.ismir.net/ismir2021/latebreaking/000005.pdf},\n",
        "}\n",
        "$$\n",
        "\n",
        "This notebook shows how to train a model (GPT2) and generate music from it, using the Hugging Face Transformers package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOd93yV0sGd2"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "***Install all dependencies (run only once per session)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fX12Yquyuihc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c498f85-7aa1-4306-dde6-cfefdb352b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 14 05:38:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Collecting miditok\n",
            "  Downloading miditok-2.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.19 in /usr/local/lib/python3.10/dist-packages (from miditok) (1.23.5)\n",
            "Collecting miditoolkit>=0.1.16 (from miditok)\n",
            "  Downloading miditoolkit-0.1.16-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from miditok) (4.66.0)\n",
            "Collecting tokenizers>=0.13.0 (from miditok)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from miditok) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from miditok) (3.7.1)\n",
            "Collecting mido>=1.1.16 (from miditoolkit>=0.1.16->miditok)\n",
            "  Downloading mido-1.3.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->miditok) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->miditok) (1.16.0)\n",
            "Installing collected packages: tokenizers, mido, miditoolkit, miditok\n",
            "Successfully installed miditok-2.1.2 miditoolkit-0.1.16 mido-1.3.0 tokenizers-0.13.3\n",
            "Requirement already satisfied: miditoolkit in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from miditoolkit) (1.23.5)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.10/dist-packages (from miditoolkit) (1.3.0)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from mido>=1.1.16->miditoolkit) (23.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting torchtoolkit\n",
            "  Downloading torchtoolkit-0.0.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchtoolkit) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from torchtoolkit) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtoolkit) (4.66.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchtoolkit) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchtoolkit) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchtoolkit) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchtoolkit) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchtoolkit) (1.3.0)\n",
            "Installing collected packages: torchtoolkit\n",
            "Successfully installed torchtoolkit-0.0.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 transformers-4.31.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.0)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 multiprocess-0.70.15 responses-0.18.0 xxhash-3.3.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!nvidia-smi\n",
        "\n",
        "!pip install miditok\n",
        "!pip install miditoolkit\n",
        "!pip install torch\n",
        "!pip install torchtoolkit\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install tqdm\n",
        "!pip install accelerate -U\n",
        "\n",
        "\n",
        "from typing import List, Tuple, Dict, Callable, Any, Union\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import json\n",
        "\n",
        "from torch import Tensor, LongTensor, stack, flip, cat, full, argmax\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtoolkit.data import create_subsets\n",
        "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
        "from transformers.data.data_collator import DataCollatorMixin\n",
        "from evaluate import load as load_metric\n",
        "from miditok import REMI, MIDITokenizer, TokenizerConfig\n",
        "from miditok.constants import CHORD_MAPS\n",
        "from miditoolkit import MidiFile\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XmGPNV8Chr_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30a0f3a-32df-450e-9f1d-8df4a7e91525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copytree('drive/MyDrive/Colab_Data/dataset','dataset/', )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZKkXnu0nSBKr",
        "outputId": "8bc14c7d-b614-4b07-cdac-25ba71b01144"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataset/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAnzrxNHKIWx"
      },
      "source": [
        "## Define Class for Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qEiUIBy1KIWx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MIDIDataset(Dataset):\n",
        "    r\"\"\"Dataset for generator training\n",
        "\n",
        "    :param files_paths: list of paths to files to load.\n",
        "    :param tokenizer: tokenizer object, to use to load MIDIs instead of tokens. (default: None)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, files_paths: List[Path], min_seq_len: int, max_seq_len: int, tokenizer: MIDITokenizer = None):\n",
        "        samples = []\n",
        "\n",
        "        for file_path in tqdm(files_paths, desc=f'Loading data: {files_paths[0].parent}'):\n",
        "            if file_path.suffix in [\"mid\", \"midi\", \"MID\", \"MIDI\"]:\n",
        "                midi = MidiFile(file_path)\n",
        "                for _ in range(len(midi.instruments) - 1):\n",
        "                    del midi.instruments[1]  # removes all tracks except first one\n",
        "                tokens = tokenizer.midi_to_tokens(midi)[0].ids\n",
        "            else:\n",
        "                with open(file_path) as json_file:\n",
        "                    tokens = json.load(json_file)['ids'][0]  # first track\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                if i >= len(tokens) - min_seq_len:\n",
        "                    break  # last sample is too short\n",
        "                samples.append(LongTensor(tokens[i:i + max_seq_len]))\n",
        "                i += len(samples[-1])  # could be replaced with max_seq_len\n",
        "\n",
        "        self.samples = samples\n",
        "\n",
        "    def __getitem__(self, idx) -> Dict[str, LongTensor]:\n",
        "        return {\"input_ids\": self.samples[idx], \"labels\": self.samples[idx]}\n",
        "\n",
        "    def __len__(self) -> int: return len(self.samples)\n",
        "\n",
        "    def __repr__(self): return self.__str__()\n",
        "\n",
        "    def __str__(self) -> str: return 'No data loaded' if len(self) == 0 else f'{len(self.samples)} samples'\n",
        "\n",
        "\n",
        "def _pad_batch(examples: List[Dict[str, LongTensor]], pad_token: int) -> LongTensor:\n",
        "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
        "\n",
        "    length_of_first = examples[0][\"input_ids\"].size(0)\n",
        "\n",
        "    # Check if padding is necessary.\n",
        "    are_tensors_same_length = all(x[\"input_ids\"].size(0) == length_of_first for x in examples)\n",
        "    if are_tensors_same_length:\n",
        "        return stack([e[\"input_ids\"] for e in examples], dim=0).long()\n",
        "\n",
        "    # Creating the full tensor and filling it with our data.\n",
        "    return pad_sequence([e[\"input_ids\"] for e in examples], batch_first=True, padding_value=pad_token).long()\n",
        "\n",
        "\n",
        "class DataCollatorGen(DataCollatorMixin):\n",
        "    def __init__(self, pad_token: int, return_tensors: str = \"pt\"):\n",
        "        \"\"\"Collator that simply pad the input sequences.\n",
        "        Input_ids will be padded with the pad token given, while labels will be\n",
        "        padded with -100.\n",
        "\n",
        "        :param pad_token: pas token\n",
        "        :param return_tensors:\n",
        "        \"\"\"\n",
        "        self.pad_token = pad_token\n",
        "        self.return_tensors = return_tensors\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Any]], return_tensors=None) -> Dict[str, LongTensor]:\n",
        "        x, y = _pad_batch(batch, self.pad_token), _pad_batch(batch, -100)\n",
        "        return {\"input_ids\": x, \"labels\": y}  # will be shifted in GPT2LMHead forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgrsoeX-LsHy"
      },
      "source": [
        "## Convert MIDI files to tokens, and load them for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jHHcxzMqLsHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2dbb75-ee14-4e5e-f56c-22d0c4c1b5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing MIDIs (Midi_tokens_no_bpe): 100%|██████████| 108/108 [00:02<00:00, 36.42it/s]\n",
            "Performing data augmentation: 100%|██████████| 108/108 [00:01<00:00, 85.30it/s]\n",
            "Loading token files: 100%|██████████| 1425/1425 [00:00<00:00, 4158.06it/s]\n",
            "Applying BPE to dataset: 100%|██████████| 1425/1425 [00:03<00:00, 421.82it/s]\n",
            "Loading data: Midi_tokens_bpe: 100%|██████████| 1425/1425 [00:00<00:00, 6215.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# Our parameters for Tokenizer\n",
        "TOKENIZER_PARAMS = {\n",
        "    \"pitch_range\": (21, 109),\n",
        "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
        "    \"nb_velocities\": 32,\n",
        "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
        "    \"use_chords\": False,\n",
        "    \"use_rests\": True,\n",
        "    \"use_tempos\": True,\n",
        "    \"use_time_signatures\": False,\n",
        "    \"use_programs\": True,\n",
        "    \"nb_tempos\": 32,  # nb of tempo bins\n",
        "    \"tempo_range\": (40, 250),  # (min, max)\n",
        "}\n",
        "# Define config for Tokenizer\n",
        "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
        "\n",
        "# Creates the tokenizer with REMI\n",
        "tokenizer = REMI(config)\n",
        "\n",
        "# config = TokenizerConfig(nb_velocities=16, use_chords=False, use_rests=True)\n",
        "\n",
        "# Path to Midis w/o Byte Pair Encoding\n",
        "tokens_no_bpe_path = Path('Midi_tokens_no_bpe')\n",
        "# Path to Midis w Byte Pair Encoding\n",
        "tokens_bpe_path = Path('Midi_tokens_bpe')\n",
        "\n",
        "# Path to input Midi files\n",
        "midi_path = Path('dataset')\n",
        "\n",
        "# List of all input Midi files\n",
        "midi_paths = list(midi_path.glob('**/*.mid')) + list(midi_path.glob('**/*.midi'))\n",
        "\n",
        "# Perform data augmentation on a whole dataset\n",
        "#data_augmentation_offsets = [2, 1, 1]  # data augmentation on 2 pitch octaves, 1 velocity and 1 duration values\n",
        "data_augmentation_offsets = [3, 2, 2]  # data augmentation on 2 pitch octaves, 1 velocity and 1 duration values\n",
        "\n",
        "# convert MIDIs to tokens\n",
        "tokenizer.tokenize_midi_dataset(midi_paths, tokens_no_bpe_path,data_augment_offsets=data_augmentation_offsets)\n",
        "\n",
        "# Learn and apply BPE to data we just tokenized\n",
        "tokens_bpe_path.mkdir(exist_ok=True, parents=True)\n",
        "tokenizer.learn_bpe(\n",
        "    #vocab_size=10000,\n",
        "    vocab_size=1000,\n",
        "    tokens_paths=list(tokens_no_bpe_path.glob(\"**/*.json\")),\n",
        "    start_from_empty_voc=False,\n",
        ")\n",
        "tokenizer.apply_bpe_to_dataset(\n",
        "    tokens_no_bpe_path,\n",
        "    tokens_bpe_path,\n",
        ")\n",
        "\n",
        "# Saving our tokenizer, to retrieve it back later with the load_params method\n",
        "tokenizer.save_params(Path(\"tokenizer\", \"tokenizer.json\"))\n",
        "\n",
        "\n",
        "# Loads tokens and create data loaders for training\n",
        "tokens_paths = list(tokens_bpe_path.glob(\"**/*.json\"))\n",
        "dataset = MIDIDataset(\n",
        "    tokens_paths, max_seq_len=256, min_seq_len=128,\n",
        ")\n",
        "subset_train, subset_valid = create_subsets(dataset, [0.3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5efYqc4LsHy"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "We will use the [GPT2 implementation of Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2). This\n",
        "Feel free to explore the documentation and source code to dig deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X77nSDSELsHz"
      },
      "outputs": [],
      "source": [
        "# Creates model\n",
        "config = GPT2Config(\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_positions=2048,\n",
        "    n_embd=512,\n",
        "    n_layer=8,\n",
        "    n_head=8,\n",
        "    n_inner=2048,\n",
        "    resid_pdrop=.1,\n",
        "    embd_pdrop=.1,\n",
        "    attn_pdrop=.1,\n",
        "    padding_token_id=tokenizer['PAD_None'],\n",
        "    bos_token_id=tokenizer['BOS_None'],\n",
        "    eos_token_id=tokenizer['EOS_None'],\n",
        ")\n",
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbhU8jwELsHz"
      },
      "source": [
        "## Train it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mvOg2w--LsHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "612ea599-f7da-4578-b7dc-16ded6108a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Currently training with a batch size of: 16\n",
            "***** Running training *****\n",
            "  Num examples = 1,957\n",
            "  Num Epochs = 2,440\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "  Gradient Accumulation steps = 3\n",
            "  Total optimization steps = 100,000\n",
            "  Number of trainable parameters = 26,780,672\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100000/100000 9:42:32, Epoch 2439/2440]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.104400</td>\n",
              "      <td>4.731550</td>\n",
              "      <td>0.000662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.361300</td>\n",
              "      <td>3.979230</td>\n",
              "      <td>0.000031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.641400</td>\n",
              "      <td>3.383149</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.936500</td>\n",
              "      <td>2.560403</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.168600</td>\n",
              "      <td>2.038081</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.659000</td>\n",
              "      <td>1.762542</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.263900</td>\n",
              "      <td>1.553847</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.911900</td>\n",
              "      <td>1.366958</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.613400</td>\n",
              "      <td>1.218740</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.396600</td>\n",
              "      <td>1.107481</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.258600</td>\n",
              "      <td>1.051055</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.178300</td>\n",
              "      <td>1.012355</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.130300</td>\n",
              "      <td>0.992143</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.097100</td>\n",
              "      <td>0.981756</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.076000</td>\n",
              "      <td>0.991569</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.065800</td>\n",
              "      <td>0.958999</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.056700</td>\n",
              "      <td>0.973079</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.052300</td>\n",
              "      <td>0.999199</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.046900</td>\n",
              "      <td>0.978571</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.044500</td>\n",
              "      <td>0.961351</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.989011</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.040300</td>\n",
              "      <td>0.945760</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.037300</td>\n",
              "      <td>0.933873</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.036100</td>\n",
              "      <td>0.920553</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.030600</td>\n",
              "      <td>0.970111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.032100</td>\n",
              "      <td>0.950587</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.035500</td>\n",
              "      <td>0.937059</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.026500</td>\n",
              "      <td>0.932557</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.029200</td>\n",
              "      <td>0.918827</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>0.912396</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.026600</td>\n",
              "      <td>0.890100</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.026800</td>\n",
              "      <td>0.893307</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.022200</td>\n",
              "      <td>0.899279</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.023500</td>\n",
              "      <td>0.995012</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.022700</td>\n",
              "      <td>0.885557</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>0.892454</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.021200</td>\n",
              "      <td>0.874310</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>0.857067</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.020100</td>\n",
              "      <td>0.890392</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.019300</td>\n",
              "      <td>0.856135</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>0.019800</td>\n",
              "      <td>0.851947</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>0.017800</td>\n",
              "      <td>0.868303</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>0.018800</td>\n",
              "      <td>0.851910</td>\n",
              "      <td>0.000025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>0.863217</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>0.017300</td>\n",
              "      <td>0.862695</td>\n",
              "      <td>0.000025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>0.017800</td>\n",
              "      <td>0.839575</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>0.016200</td>\n",
              "      <td>0.849346</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>0.018800</td>\n",
              "      <td>0.824641</td>\n",
              "      <td>0.000025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>0.014900</td>\n",
              "      <td>0.824877</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>0.015700</td>\n",
              "      <td>0.857043</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.838014</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>0.015500</td>\n",
              "      <td>0.843085</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>0.015400</td>\n",
              "      <td>0.906021</td>\n",
              "      <td>0.000046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>0.016400</td>\n",
              "      <td>0.817407</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>0.014400</td>\n",
              "      <td>0.840431</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>0.014700</td>\n",
              "      <td>0.830114</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>0.832827</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>0.014300</td>\n",
              "      <td>0.833087</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.830080</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>0.014100</td>\n",
              "      <td>0.828191</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61000</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.825808</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62000</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.822795</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63000</td>\n",
              "      <td>0.013600</td>\n",
              "      <td>0.823178</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64000</td>\n",
              "      <td>0.013500</td>\n",
              "      <td>0.826948</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65000</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.830041</td>\n",
              "      <td>0.000025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66000</td>\n",
              "      <td>0.013100</td>\n",
              "      <td>0.834902</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67000</td>\n",
              "      <td>0.014300</td>\n",
              "      <td>0.808295</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68000</td>\n",
              "      <td>0.012700</td>\n",
              "      <td>0.810903</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69000</td>\n",
              "      <td>0.012700</td>\n",
              "      <td>0.816806</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70000</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>0.823695</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71000</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>0.820219</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72000</td>\n",
              "      <td>0.012700</td>\n",
              "      <td>0.812174</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73000</td>\n",
              "      <td>0.012700</td>\n",
              "      <td>0.820632</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74000</td>\n",
              "      <td>0.012600</td>\n",
              "      <td>0.814657</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75000</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.820033</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76000</td>\n",
              "      <td>0.012400</td>\n",
              "      <td>0.822575</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77000</td>\n",
              "      <td>0.012400</td>\n",
              "      <td>0.855165</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78000</td>\n",
              "      <td>0.012400</td>\n",
              "      <td>0.819941</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79000</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.823488</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80000</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.816817</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81000</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.813376</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82000</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.817822</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>0.811384</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>0.811392</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>0.817045</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86000</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.811722</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87000</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.814450</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88000</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.809987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89000</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.808519</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90000</td>\n",
              "      <td>0.011800</td>\n",
              "      <td>0.811576</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91000</td>\n",
              "      <td>0.011800</td>\n",
              "      <td>0.810828</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92000</td>\n",
              "      <td>0.011800</td>\n",
              "      <td>0.810647</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93000</td>\n",
              "      <td>0.011800</td>\n",
              "      <td>0.809016</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94000</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.809024</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95000</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.808709</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96000</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.808859</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97000</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.808865</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98000</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.808818</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99000</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.808818</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100000</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.808818</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-1000\n",
            "Configuration saved in runs/checkpoint-1000/config.json\n",
            "Configuration saved in runs/checkpoint-1000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-2000\n",
            "Configuration saved in runs/checkpoint-2000/config.json\n",
            "Configuration saved in runs/checkpoint-2000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-2000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-3000\n",
            "Configuration saved in runs/checkpoint-3000/config.json\n",
            "Configuration saved in runs/checkpoint-3000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-4000\n",
            "Configuration saved in runs/checkpoint-4000/config.json\n",
            "Configuration saved in runs/checkpoint-4000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-4000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-5000\n",
            "Configuration saved in runs/checkpoint-5000/config.json\n",
            "Configuration saved in runs/checkpoint-5000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-5000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-6000\n",
            "Configuration saved in runs/checkpoint-6000/config.json\n",
            "Configuration saved in runs/checkpoint-6000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-6000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-1000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-7000\n",
            "Configuration saved in runs/checkpoint-7000/config.json\n",
            "Configuration saved in runs/checkpoint-7000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-7000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-2000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-8000\n",
            "Configuration saved in runs/checkpoint-8000/config.json\n",
            "Configuration saved in runs/checkpoint-8000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-8000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-3000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-9000\n",
            "Configuration saved in runs/checkpoint-9000/config.json\n",
            "Configuration saved in runs/checkpoint-9000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-9000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-4000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-10000\n",
            "Configuration saved in runs/checkpoint-10000/config.json\n",
            "Configuration saved in runs/checkpoint-10000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-10000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-5000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-11000\n",
            "Configuration saved in runs/checkpoint-11000/config.json\n",
            "Configuration saved in runs/checkpoint-11000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-11000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-6000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-12000\n",
            "Configuration saved in runs/checkpoint-12000/config.json\n",
            "Configuration saved in runs/checkpoint-12000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-12000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-7000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-13000\n",
            "Configuration saved in runs/checkpoint-13000/config.json\n",
            "Configuration saved in runs/checkpoint-13000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-13000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-8000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-14000\n",
            "Configuration saved in runs/checkpoint-14000/config.json\n",
            "Configuration saved in runs/checkpoint-14000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-14000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-9000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-15000\n",
            "Configuration saved in runs/checkpoint-15000/config.json\n",
            "Configuration saved in runs/checkpoint-15000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-15000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-10000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-16000\n",
            "Configuration saved in runs/checkpoint-16000/config.json\n",
            "Configuration saved in runs/checkpoint-16000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-16000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-11000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-17000\n",
            "Configuration saved in runs/checkpoint-17000/config.json\n",
            "Configuration saved in runs/checkpoint-17000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-17000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-12000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-18000\n",
            "Configuration saved in runs/checkpoint-18000/config.json\n",
            "Configuration saved in runs/checkpoint-18000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-18000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-13000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-19000\n",
            "Configuration saved in runs/checkpoint-19000/config.json\n",
            "Configuration saved in runs/checkpoint-19000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-19000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-14000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-20000\n",
            "Configuration saved in runs/checkpoint-20000/config.json\n",
            "Configuration saved in runs/checkpoint-20000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-20000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-15000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-21000\n",
            "Configuration saved in runs/checkpoint-21000/config.json\n",
            "Configuration saved in runs/checkpoint-21000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-21000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-17000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-22000\n",
            "Configuration saved in runs/checkpoint-22000/config.json\n",
            "Configuration saved in runs/checkpoint-22000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-22000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-16000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-23000\n",
            "Configuration saved in runs/checkpoint-23000/config.json\n",
            "Configuration saved in runs/checkpoint-23000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-23000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-18000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-24000\n",
            "Configuration saved in runs/checkpoint-24000/config.json\n",
            "Configuration saved in runs/checkpoint-24000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-24000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-19000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-25000\n",
            "Configuration saved in runs/checkpoint-25000/config.json\n",
            "Configuration saved in runs/checkpoint-25000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-25000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-20000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-26000\n",
            "Configuration saved in runs/checkpoint-26000/config.json\n",
            "Configuration saved in runs/checkpoint-26000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-26000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-21000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-27000\n",
            "Configuration saved in runs/checkpoint-27000/config.json\n",
            "Configuration saved in runs/checkpoint-27000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-27000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-22000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-28000\n",
            "Configuration saved in runs/checkpoint-28000/config.json\n",
            "Configuration saved in runs/checkpoint-28000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-28000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-23000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-29000\n",
            "Configuration saved in runs/checkpoint-29000/config.json\n",
            "Configuration saved in runs/checkpoint-29000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-29000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-24000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-30000\n",
            "Configuration saved in runs/checkpoint-30000/config.json\n",
            "Configuration saved in runs/checkpoint-30000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-30000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-25000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-31000\n",
            "Configuration saved in runs/checkpoint-31000/config.json\n",
            "Configuration saved in runs/checkpoint-31000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-31000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-26000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-32000\n",
            "Configuration saved in runs/checkpoint-32000/config.json\n",
            "Configuration saved in runs/checkpoint-32000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-32000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-27000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-33000\n",
            "Configuration saved in runs/checkpoint-33000/config.json\n",
            "Configuration saved in runs/checkpoint-33000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-33000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-28000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-34000\n",
            "Configuration saved in runs/checkpoint-34000/config.json\n",
            "Configuration saved in runs/checkpoint-34000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-34000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-29000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-35000\n",
            "Configuration saved in runs/checkpoint-35000/config.json\n",
            "Configuration saved in runs/checkpoint-35000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-35000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-30000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-36000\n",
            "Configuration saved in runs/checkpoint-36000/config.json\n",
            "Configuration saved in runs/checkpoint-36000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-36000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-31000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-37000\n",
            "Configuration saved in runs/checkpoint-37000/config.json\n",
            "Configuration saved in runs/checkpoint-37000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-37000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-32000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-38000\n",
            "Configuration saved in runs/checkpoint-38000/config.json\n",
            "Configuration saved in runs/checkpoint-38000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-38000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-33000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-39000\n",
            "Configuration saved in runs/checkpoint-39000/config.json\n",
            "Configuration saved in runs/checkpoint-39000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-39000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-34000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-40000\n",
            "Configuration saved in runs/checkpoint-40000/config.json\n",
            "Configuration saved in runs/checkpoint-40000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-40000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-35000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-41000\n",
            "Configuration saved in runs/checkpoint-41000/config.json\n",
            "Configuration saved in runs/checkpoint-41000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-41000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-36000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-42000\n",
            "Configuration saved in runs/checkpoint-42000/config.json\n",
            "Configuration saved in runs/checkpoint-42000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-42000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-37000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-43000\n",
            "Configuration saved in runs/checkpoint-43000/config.json\n",
            "Configuration saved in runs/checkpoint-43000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-43000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-38000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-44000\n",
            "Configuration saved in runs/checkpoint-44000/config.json\n",
            "Configuration saved in runs/checkpoint-44000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-44000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-39000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-45000\n",
            "Configuration saved in runs/checkpoint-45000/config.json\n",
            "Configuration saved in runs/checkpoint-45000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-45000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-40000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-46000\n",
            "Configuration saved in runs/checkpoint-46000/config.json\n",
            "Configuration saved in runs/checkpoint-46000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-46000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-41000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-47000\n",
            "Configuration saved in runs/checkpoint-47000/config.json\n",
            "Configuration saved in runs/checkpoint-47000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-47000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-42000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-48000\n",
            "Configuration saved in runs/checkpoint-48000/config.json\n",
            "Configuration saved in runs/checkpoint-48000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-48000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-43000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-49000\n",
            "Configuration saved in runs/checkpoint-49000/config.json\n",
            "Configuration saved in runs/checkpoint-49000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-49000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-44000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-50000\n",
            "Configuration saved in runs/checkpoint-50000/config.json\n",
            "Configuration saved in runs/checkpoint-50000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-50000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-45000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-51000\n",
            "Configuration saved in runs/checkpoint-51000/config.json\n",
            "Configuration saved in runs/checkpoint-51000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-51000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-46000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-52000\n",
            "Configuration saved in runs/checkpoint-52000/config.json\n",
            "Configuration saved in runs/checkpoint-52000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-52000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-47000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-53000\n",
            "Configuration saved in runs/checkpoint-53000/config.json\n",
            "Configuration saved in runs/checkpoint-53000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-53000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-49000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-54000\n",
            "Configuration saved in runs/checkpoint-54000/config.json\n",
            "Configuration saved in runs/checkpoint-54000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-54000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-48000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-55000\n",
            "Configuration saved in runs/checkpoint-55000/config.json\n",
            "Configuration saved in runs/checkpoint-55000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-55000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-50000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-56000\n",
            "Configuration saved in runs/checkpoint-56000/config.json\n",
            "Configuration saved in runs/checkpoint-56000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-56000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-51000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-57000\n",
            "Configuration saved in runs/checkpoint-57000/config.json\n",
            "Configuration saved in runs/checkpoint-57000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-57000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-52000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-58000\n",
            "Configuration saved in runs/checkpoint-58000/config.json\n",
            "Configuration saved in runs/checkpoint-58000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-58000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-53000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-59000\n",
            "Configuration saved in runs/checkpoint-59000/config.json\n",
            "Configuration saved in runs/checkpoint-59000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-59000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-55000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-60000\n",
            "Configuration saved in runs/checkpoint-60000/config.json\n",
            "Configuration saved in runs/checkpoint-60000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-60000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-56000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-61000\n",
            "Configuration saved in runs/checkpoint-61000/config.json\n",
            "Configuration saved in runs/checkpoint-61000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-61000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-57000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-62000\n",
            "Configuration saved in runs/checkpoint-62000/config.json\n",
            "Configuration saved in runs/checkpoint-62000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-62000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-58000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-63000\n",
            "Configuration saved in runs/checkpoint-63000/config.json\n",
            "Configuration saved in runs/checkpoint-63000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-63000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-59000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-64000\n",
            "Configuration saved in runs/checkpoint-64000/config.json\n",
            "Configuration saved in runs/checkpoint-64000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-64000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-60000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-65000\n",
            "Configuration saved in runs/checkpoint-65000/config.json\n",
            "Configuration saved in runs/checkpoint-65000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-65000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-61000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-66000\n",
            "Configuration saved in runs/checkpoint-66000/config.json\n",
            "Configuration saved in runs/checkpoint-66000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-66000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-62000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-67000\n",
            "Configuration saved in runs/checkpoint-67000/config.json\n",
            "Configuration saved in runs/checkpoint-67000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-67000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-54000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-68000\n",
            "Configuration saved in runs/checkpoint-68000/config.json\n",
            "Configuration saved in runs/checkpoint-68000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-68000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-63000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-69000\n",
            "Configuration saved in runs/checkpoint-69000/config.json\n",
            "Configuration saved in runs/checkpoint-69000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-69000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-64000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-70000\n",
            "Configuration saved in runs/checkpoint-70000/config.json\n",
            "Configuration saved in runs/checkpoint-70000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-70000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-65000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-71000\n",
            "Configuration saved in runs/checkpoint-71000/config.json\n",
            "Configuration saved in runs/checkpoint-71000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-71000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-66000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-72000\n",
            "Configuration saved in runs/checkpoint-72000/config.json\n",
            "Configuration saved in runs/checkpoint-72000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-72000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-68000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-73000\n",
            "Configuration saved in runs/checkpoint-73000/config.json\n",
            "Configuration saved in runs/checkpoint-73000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-73000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-69000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-74000\n",
            "Configuration saved in runs/checkpoint-74000/config.json\n",
            "Configuration saved in runs/checkpoint-74000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-74000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-70000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-75000\n",
            "Configuration saved in runs/checkpoint-75000/config.json\n",
            "Configuration saved in runs/checkpoint-75000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-75000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-71000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-76000\n",
            "Configuration saved in runs/checkpoint-76000/config.json\n",
            "Configuration saved in runs/checkpoint-76000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-76000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-72000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-77000\n",
            "Configuration saved in runs/checkpoint-77000/config.json\n",
            "Configuration saved in runs/checkpoint-77000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-77000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-73000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-78000\n",
            "Configuration saved in runs/checkpoint-78000/config.json\n",
            "Configuration saved in runs/checkpoint-78000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-78000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-74000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-79000\n",
            "Configuration saved in runs/checkpoint-79000/config.json\n",
            "Configuration saved in runs/checkpoint-79000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-79000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-75000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-80000\n",
            "Configuration saved in runs/checkpoint-80000/config.json\n",
            "Configuration saved in runs/checkpoint-80000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-80000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-76000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-81000\n",
            "Configuration saved in runs/checkpoint-81000/config.json\n",
            "Configuration saved in runs/checkpoint-81000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-81000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-77000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-82000\n",
            "Configuration saved in runs/checkpoint-82000/config.json\n",
            "Configuration saved in runs/checkpoint-82000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-82000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-78000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-83000\n",
            "Configuration saved in runs/checkpoint-83000/config.json\n",
            "Configuration saved in runs/checkpoint-83000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-83000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-79000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-84000\n",
            "Configuration saved in runs/checkpoint-84000/config.json\n",
            "Configuration saved in runs/checkpoint-84000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-84000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-80000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-85000\n",
            "Configuration saved in runs/checkpoint-85000/config.json\n",
            "Configuration saved in runs/checkpoint-85000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-85000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-81000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-86000\n",
            "Configuration saved in runs/checkpoint-86000/config.json\n",
            "Configuration saved in runs/checkpoint-86000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-86000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-82000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-87000\n",
            "Configuration saved in runs/checkpoint-87000/config.json\n",
            "Configuration saved in runs/checkpoint-87000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-87000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-83000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-88000\n",
            "Configuration saved in runs/checkpoint-88000/config.json\n",
            "Configuration saved in runs/checkpoint-88000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-88000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-84000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-89000\n",
            "Configuration saved in runs/checkpoint-89000/config.json\n",
            "Configuration saved in runs/checkpoint-89000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-89000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-85000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-90000\n",
            "Configuration saved in runs/checkpoint-90000/config.json\n",
            "Configuration saved in runs/checkpoint-90000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-90000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-86000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-91000\n",
            "Configuration saved in runs/checkpoint-91000/config.json\n",
            "Configuration saved in runs/checkpoint-91000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-91000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-87000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-92000\n",
            "Configuration saved in runs/checkpoint-92000/config.json\n",
            "Configuration saved in runs/checkpoint-92000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-92000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-88000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-93000\n",
            "Configuration saved in runs/checkpoint-93000/config.json\n",
            "Configuration saved in runs/checkpoint-93000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-93000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-89000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-94000\n",
            "Configuration saved in runs/checkpoint-94000/config.json\n",
            "Configuration saved in runs/checkpoint-94000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-94000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-90000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-95000\n",
            "Configuration saved in runs/checkpoint-95000/config.json\n",
            "Configuration saved in runs/checkpoint-95000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-95000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-91000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-96000\n",
            "Configuration saved in runs/checkpoint-96000/config.json\n",
            "Configuration saved in runs/checkpoint-96000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-96000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-92000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-97000\n",
            "Configuration saved in runs/checkpoint-97000/config.json\n",
            "Configuration saved in runs/checkpoint-97000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-97000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-93000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-98000\n",
            "Configuration saved in runs/checkpoint-98000/config.json\n",
            "Configuration saved in runs/checkpoint-98000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-98000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-94000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-99000\n",
            "Configuration saved in runs/checkpoint-99000/config.json\n",
            "Configuration saved in runs/checkpoint-99000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-99000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-95000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 838\n",
            "  Batch size = 48\n",
            "Saving model checkpoint to runs/checkpoint-100000\n",
            "Configuration saved in runs/checkpoint-100000/config.json\n",
            "Configuration saved in runs/checkpoint-100000/generation_config.json\n",
            "Model weights saved in runs/checkpoint-100000/pytorch_model.bin\n",
            "Deleting older checkpoint [runs/checkpoint-96000] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from runs/checkpoint-67000 (score: 0.8082954287528992).\n",
            "Saving model checkpoint to runs\n",
            "Configuration saved in runs/config.json\n",
            "Configuration saved in runs/generation_config.json\n",
            "Model weights saved in runs/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =     2439.02\n",
            "  total_flos               = 172204656GF\n",
            "  train_loss               =      0.2545\n",
            "  train_runtime            =  9:42:32.98\n",
            "  train_samples_per_second =     137.327\n",
            "  train_steps_per_second   =       2.861\n"
          ]
        }
      ],
      "source": [
        "metrics = {metric: load_metric(metric) for metric in [\"accuracy\"]}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes metrics for pretraining.\n",
        "    Must use proprocess_logits function that converts logits to predictions (argmax or sampling).\n",
        "\n",
        "    :param eval_pred: EvalPrediction containing predictions and labels\n",
        "    :return: metrics\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    not_pad_mask = labels != -100\n",
        "    labels, predictions = labels[not_pad_mask], predictions[not_pad_mask]\n",
        "    return metrics[\"accuracy\"].compute(predictions=predictions.flatten(), references=labels.flatten())\n",
        "\n",
        "def preprocess_logits(logits: Tensor, _: Tensor) -> Tensor:\n",
        "    \"\"\"Preprocesses the logits before accumulating them during evaluation.\n",
        "    This allows to significantly reduce the memory usage and make the training tractable.\n",
        "    \"\"\"\n",
        "    pred_ids = argmax(logits, dim=-1)  # long dtype\n",
        "    return pred_ids\n",
        "\n",
        "training_config = TrainingArguments(\n",
        "    \"runs\", False, True, True, False, \"steps\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=48,\n",
        "    gradient_accumulation_steps=3,\n",
        "    eval_accumulation_steps=None,\n",
        "    eval_steps=1000,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=3.0,\n",
        "    max_steps=100000,\n",
        "    lr_scheduler_type=\"cosine_with_restarts\",\n",
        "    warmup_ratio=0.3,\n",
        "    log_level=\"debug\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1000,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    save_total_limit=5,\n",
        "    no_cuda=False,\n",
        "    seed=444,\n",
        "    fp16=False,\n",
        "    load_best_model_at_end=True,\n",
        "    label_smoothing_factor=0.,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=[\"tensorboard\"],\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_config,\n",
        "    data_collator=DataCollatorGen(tokenizer[\"PAD_None\"]),\n",
        "    train_dataset=subset_train,\n",
        "    eval_dataset=subset_valid,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=None,\n",
        "    preprocess_logits_for_metrics=preprocess_logits,\n",
        ")\n",
        "\n",
        "# Training\n",
        "train_result = trainer.train()\n",
        "trainer.save_model()  # Saves the tokenizer too\n",
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFhNMbuLsHz"
      },
      "source": [
        "## Generate music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "OaNkGcFo9UP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d410567-7c82-4135-b13b-8a5f57de7db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing model / Generating results: 100%|██████████| 53/53 [02:32<00:00,  2.87s/it]\n"
          ]
        }
      ],
      "source": [
        "def collate_gen_left(batch: List[Dict[str, LongTensor]]) -> LongTensor:\n",
        "    # Here the sequences are padded to the left, so that the last token along the time dimension\n",
        "    # is always the last token of each seq, allowing to efficiently generate by batch\n",
        "    bos_shape = (1,)\n",
        "    batch = [flip(cat([full(bos_shape, tokenizer[\"BOS_None\"]), seq[\"input_ids\"]], dim=0), dims=(0,)) for seq in batch]\n",
        "    batch = pad_sequence(batch, batch_first=True, padding_value=tokenizer[\"PAD_None\"])  # (N,T) or (N,T,Z)\n",
        "    batch = flip(batch, dims=(1,)).long()\n",
        "    return batch  # (N,T)\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=256,  # extends samples by 512 tokens\n",
        "    num_beams=1,        # no beam search\n",
        "    do_sample=True,     # but sample instead\n",
        "    temperature=0.9,\n",
        "    top_k=15,\n",
        "    top_p=0.95,\n",
        "    epsilon_cutoff=3e-4,\n",
        "    eta_cutoff=1e-3,\n",
        "    pad_token_id=config.padding_token_id,\n",
        ")\n",
        "\n",
        "(gen_results_path := Path('gen_res')).mkdir(parents=True, exist_ok=True)\n",
        "dataloader_test = DataLoader(subset_valid, batch_size=16, collate_fn=collate_gen_left)\n",
        "model.eval()\n",
        "count = 0\n",
        "for batch in tqdm(dataloader_test, desc='Testing model / Generating results'):  # (N,T)\n",
        "    res = model.generate(batch.to(model.device), generation_config=generation_config)  # (N,T)\n",
        "\n",
        "    # Saves the generated music, as MIDI files and tokens (json)\n",
        "    for prompt, continuation in zip(batch, res):\n",
        "        generated = continuation[len(prompt):]\n",
        "        tokens = [generated, prompt, continuation]  # list compr. as seqs of dif. lengths\n",
        "        tokens = [seq.tolist() for seq in tokens]\n",
        "        midi = tokenizer.tokens_to_midi(deepcopy(tokens), time_division=384)\n",
        "        midi.instruments[0].name = f'Continuation of original sample ({len(generated)} tokens)'\n",
        "        midi.instruments[1].name = f'Original sample ({len(prompt)} tokens)'\n",
        "        midi.instruments[2].name = f'Original sample and continuation'\n",
        "        midi.dump(gen_results_path / f'{count}.mid')\n",
        "        tokenizer.save_tokens(tokens, gen_results_path / f'{count}.json')\n",
        "\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copytree('gen_res/', 'drive/MyDrive/Colab_Data/gen_res_final')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4bQOhkxqL3ZC",
        "outputId": "ec4093d3-0ab4-4aa0-abdc-16ac0f4ea69a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/Colab_Data/gen_res_final'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copytree('runs/', 'drive/MyDrive/Colab_Data/runs_final')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AVMQa4jTfIwF",
        "outputId": "f5be2f2d-4232-4592-df18-d6a521cb6d4d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/Colab_Data/runs_final'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copytree('Midi_tokens_bpe/', 'drive/MyDrive/Colab_Data/Midi_tokens_bpe_final')\n",
        "shutil.copytree('Midi_tokens_no_bpe/', 'drive/MyDrive/Colab_Data/Midi_tokens_no_bpe_final')\n",
        "shutil.copytree('tokenizer/', 'drive/MyDrive/Colab_Data/tokenizer_final')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iyQvHUJbnspO",
        "outputId": "42cd9e1e-0d22-4fca-e1ec-53066aa2b45f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/Colab_Data/tokenizer_final'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}